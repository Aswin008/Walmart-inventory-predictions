{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 18599,
          "databundleVersionId": 1236839,
          "sourceType": "competition"
        },
        {
          "sourceId": 7134990,
          "sourceType": "datasetVersion",
          "datasetId": 4116919
        }
      ],
      "dockerImageVersionId": 30587,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "EsjDRTa5lv2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import gc\n",
        "import lightgbm as lgb\n",
        "import joblib\n",
        "from lightgbm import LGBMRegressor"
      ],
      "metadata": {
        "trusted": true,
        "id": "jB8tZYk2lv2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calendar = pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/calendar.csv\")\n",
        "train_data = pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/sales_train_evaluation.csv\")\n",
        "sell_prices = pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/sell_prices.csv\")\n",
        "submission = pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/sample_submission.csv\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "EmFUQClclv2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for d in range(1942,1970):\n",
        "    col = 'd_' + str(d)\n",
        "    train_data[col] = 0\n",
        "    train_data[col] = train_data[col].astype(np.int16)"
      ],
      "metadata": {
        "trusted": true,
        "id": "KGaHS_snlv2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def downcast(df):\n",
        "    cols = df.dtypes.index.tolist()\n",
        "    types = df.dtypes.values.tolist()\n",
        "    for i,t in enumerate(types):\n",
        "        if 'int' in str(t):\n",
        "            if df[cols[i]].min() > np.iinfo(np.int8).min and df[cols[i]].max() < np.iinfo(np.int8).max:\n",
        "                df[cols[i]] = df[cols[i]].astype(np.int8)\n",
        "            elif df[cols[i]].min() > np.iinfo(np.int16).min and df[cols[i]].max() < np.iinfo(np.int16).max:\n",
        "                df[cols[i]] = df[cols[i]].astype(np.int16)\n",
        "            elif df[cols[i]].min() > np.iinfo(np.int32).min and df[cols[i]].max() < np.iinfo(np.int32).max:\n",
        "                df[cols[i]] = df[cols[i]].astype(np.int32)\n",
        "            else:\n",
        "                df[cols[i]] = df[cols[i]].astype(np.int64)\n",
        "        elif 'float' in str(t):\n",
        "            if df[cols[i]].min() > np.finfo(np.float16).min and df[cols[i]].max() < np.finfo(np.float16).max:\n",
        "                df[cols[i]] = df[cols[i]].astype(np.float16)\n",
        "            elif df[cols[i]].min() > np.finfo(np.float32).min and df[cols[i]].max() < np.finfo(np.float32).max:\n",
        "                df[cols[i]] = df[cols[i]].astype(np.float32)\n",
        "            else:\n",
        "                df[cols[i]] = df[cols[i]].astype(np.float64)\n",
        "        elif t == object:\n",
        "            if cols[i] == 'date':\n",
        "                df[cols[i]] = pd.to_datetime(df[cols[i]], format='%Y-%m-%d')\n",
        "            else:\n",
        "                df[cols[i]] = df[cols[i]].astype('category')\n",
        "    return df"
      ],
      "metadata": {
        "trusted": true,
        "id": "zj0LzJhSlv2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Downcasting data\")\n",
        "train_eva = downcast(train_data)\n",
        "sell_prices = downcast(sell_prices)\n",
        "calendar = downcast(calendar)"
      ],
      "metadata": {
        "trusted": true,
        "id": "xVlD54lXlv2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_pickle('data.pkl')\n",
        "valid = data[(data['d']>=1914) & (data['d']<1942)][['id','d','sold']]\n",
        "test = data[data['d']>=1942][['id','d','sold']]\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "004WjfNElv2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Get Labels\")\n",
        "d_id = dict(zip(train_data[\"id\"].cat.codes, train_data[\"id\"]))\n",
        "d_store = dict(zip(train_data[\"store_id\"].cat.codes, train_data[\"store_id\"]))\n",
        "d_dept = dict(zip(train_data[\"dept_id\"].cat.codes, train_data[\"dept_id\"]))\n",
        "d_cat = dict(zip(train_data[\"cat_id\"].cat.codes, train_data[\"cat_id\"]))"
      ],
      "metadata": {
        "trusted": true,
        "id": "EzZ58APplv2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bk82jnvolv2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns"
      ],
      "metadata": {
        "trusted": true,
        "id": "QOzKSrTQlv22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.drop(['sales_lag_5', 'sales_lag_8','sales_lag_10', 'sales_lag_25', 'sales_lag_26', 'sales_lag_27', 'rolling_mean_tmp_7_30',\n",
        "                  'rolling_mean_tmp_7_60', 'rolling_mean_tmp_14_7','rolling_mean_tmp_14_14', 'rolling_mean_tmp_14_30',\n",
        "                  'rolling_mean_tmp_14_60',  'rolling_mean_tmp_1_7','rolling_mean_tmp_1_14', 'rolling_mean_tmp_1_30',\n",
        "                  'rolling_mean_tmp_1_60', 'rolling_mean_tmp_7_7','rolling_mean_tmp_7_14', 'rolling_mean_tmp_7_30',\n",
        "                  'rolling_mean_tmp_7_60'], axis = 1)"
      ],
      "metadata": {
        "trusted": true,
        "id": "OX6O8Ts_lv23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create store dataframe for model predict total sale for store\n",
        "def create_dept_dataframe(train_eva, dept):\n",
        "    train = train_eva[train_eva[\"id\"].str.contains(dept)]\n",
        "\n",
        "\n",
        "    df = pd.melt(frame=train,\n",
        "                 id_vars=[\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"],\n",
        "                 var_name=\"d\", value_name=\"sold\")\n",
        "    sum_df = df.groupby([\"d\", \"dept_id\"])[\"sold\"].sum().reset_index()\n",
        "\n",
        "    df = pd.merge(left=df.drop(\"sold\", axis=1), right=sum_df, how=\"left\", on=[\"d\", \"dept_id\"])[[\"dept_id\", \"state_id\", \"d\", \"sold\"]]\n",
        "\n",
        "    df.drop_duplicates(inplace=True)\n",
        "\n",
        "    df = pd.merge(left=df, right=calendar, how=\"left\", on=\"d\")\n",
        "\n",
        "\n",
        "    df = df.drop([\"dept_id\", \"state_id\", \"date\", \"weekday\", \"event_name_2\", \"event_type_2\"], axis=1)\n",
        "    df[\"d\"] = df[\"d\"].str[2:].astype(np.int16)\n",
        "\n",
        "    df[\"event_name_1\"] = df[\"event_name_1\"].cat.codes\n",
        "    df[\"event_type_1\"] = df[\"event_type_1\"].cat.codes\n",
        "    X_train, y_train = df[df[\"d\"] < 1914].drop(\"sold\", axis=1), df[df[\"d\"] < 1914][\"sold\"]\n",
        "    lgb_train_sets = lgb.Dataset(X_train, y_train)\n",
        "    X_valid, y_valid = df[(df[\"d\"] >= 1914) & (df[\"d\"] < 1942)].drop(\"sold\", axis=1), df[(df[\"d\"] >= 1914) & (df[\"d\"] < 1942)][\"sold\"]\n",
        "    lgb_valid_sets = lgb.Dataset(X_valid, y_valid)\n",
        "    X_test, y_test = df[df[\"d\"] >= 1942].drop(\"sold\", axis=1), df[df[\"d\"] >= 1942][\"sold\"]\n",
        "\n",
        "\n",
        "    return lgb_train_sets,lgb_valid_sets, X_train, y_train, X_valid, y_valid, X_test, y_test"
      ],
      "metadata": {
        "trusted": true,
        "id": "9SENfzuklv24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_store_dataframe(train, store):\n",
        "    train = train[train[\"id\"].str.contains(store)]\n",
        "\n",
        "    final_df = pd.melt(frame=train,\n",
        "                 id_vars=[\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"],\n",
        "                 var_name=\"d\", value_name=\"sold\")\n",
        "    sum_df = final_df.groupby([\"d\", \"store_id\"])[\"sold\"].sum().reset_index()\n",
        "    final_df = pd.merge(left=final_df.drop(\"sold\", axis=1), right=sum_df, how=\"left\", on=[\"d\", \"store_id\"]) \\\n",
        "    [[\"store_id\", \"state_id\", \"d\", \"sold\"]]\n",
        "    final_df.drop_duplicates(inplace=True)\n",
        "\n",
        "    final_df = pd.merge(left=final_df, right=calendar, how=\"left\", on=\"d\")\n",
        "\n",
        "\n",
        "    final_df[\"d\"] = final_df[\"d\"].apply(lambda x: int(x[2:])).astype(np.int16)\n",
        "\n",
        "    final_df[\"event_name_1\"] = final_df[\"event_name_1\"].cat.codes\n",
        "    final_df[\"event_type_1\"] = final_df[\"event_type_1\"].cat.codes\n",
        "    final_df = final_df.drop([\"store_id\", \"state_id\", \"date\", \"weekday\", \"event_name_2\", \"event_type_2\"], axis=1)\n",
        "\n",
        "    X_train, y_train = final_df[final_df[\"d\"] < 1914].drop(\"sold\", axis=1), final_df[final_df[\"d\"] < 1914][\"sold\"]\n",
        "    lgb_train_sets = lgb.Dataset(X_train, y_train)\n",
        "    X_valid, y_valid = final_df[(final_df[\"d\"] >= 1914) & (final_df[\"d\"] < 1942)].drop(\"sold\", axis=1), final_df[(final_df[\"d\"] >= 1914) & \\\n",
        "                                                                                         (final_df[\"d\"] < 1942)][\"sold\"]\n",
        "    lgb_valid_sets = lgb.Dataset(X_valid, y_valid)\n",
        "    X_test, y_test = final_df[final_df[\"d\"] >= 1942].drop(\"sold\", axis=1), final_df[final_df[\"d\"] >= 1942][\"sold\"]\n",
        "\n",
        "\n",
        "    return lgb_train_sets,lgb_valid_sets, X_train, y_train, X_valid, y_valid, X_test, y_test"
      ],
      "metadata": {
        "id": "ZMtt8iqClv25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\"objective\" : \"tweedie\",\n",
        "        'boosting_type': 'gbdt',\n",
        "        'learning_rate': 0.034,\n",
        "        'max_depth' : 135,\n",
        "        'num_leaves': 88,\n",
        "        'n_estimators': 1000,\n",
        "        'force_row_wise':True,\n",
        "        'lambda_l2':1\n",
        "}"
      ],
      "metadata": {
        "trusted": true,
        "id": "lzGTZEmglv27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dept_model(train_data, dept_id, store_dict):\n",
        "\n",
        "    train_sets, valid_sets, X_train, y_train, X_valid, y_valid, X_test, y_test = create_dept_dataframe(train_data, dept_id)\n",
        "    new_index = y_test.index.map(lambda x: x + 1)\n",
        "    y_test.index = new_index\n",
        "\n",
        "    dept_model = lgb.train(params=params,\n",
        "                            train_set=train_sets,\n",
        "                            valid_sets=valid_sets,\n",
        "                            verbose_eval=50,\n",
        "                            early_stopping_rounds=50)\n",
        "\n",
        "    dept_preds = dept_model.predict(X_test)\n",
        "    dept_preds_map = dict(pd.Series(dept_preds, index=y_test.index))\n",
        "\n",
        "\n",
        "    return dept_preds_map"
      ],
      "metadata": {
        "trusted": true,
        "id": "LZQhz2xYlv27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oFZQ6sf8mfQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_store_model(train_data, store_id):\n",
        "    train_sets, valid_sets, X_train, y_train, X_valid, y_valid, X_test, y_test = create_store_dataframe(train_data, store_id)\n",
        "    new_index = y_test.index.map(lambda x: x + 1)\n",
        "    y_test.index = new_index\n",
        "\n",
        "    store_model = lgb.train(params=params,\n",
        "                            train_set=train_sets,\n",
        "                            valid_sets=valid_sets,\n",
        "                            verbose_eval=50,\n",
        "                            early_stopping_rounds=50)\n",
        "\n",
        "    store_preds = store_model.predict(X_test)\n",
        "    store_preds_map = dict(pd.Series(store_preds, index=y_test.index))\n",
        "\n",
        "    return store_preds_map"
      ],
      "metadata": {
        "id": "SlNhYPE0lv28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_predict(df, store_id, dept_id, store_preds_map, dept_preds_map):\n",
        "    df = data[(data[\"store_id\"] == store_id) & (data[\"dept_id\"] == dept_id)]\n",
        "    df.loc[df[\"d\"] >= 1942, \"total_sale_store\"] = df.loc[df[\"d\"] >= 1942, :][\"d\"].map(store_preds_map)\n",
        "    df.loc[df[\"d\"] >= 1942, \"total_sale_dept\"] = df.loc[df[\"d\"] >= 1942, :][\"d\"].map(dept_preds_map)\n",
        "\n",
        "    # Create train set\n",
        "    X_train, y_train = df_dept[df_dept['d'] < 1914].drop('sold', axis=1), df_dept[df_dept['d'] < 1914]['sold']\n",
        "    train_sets = lgb.Dataset(X_train, y_train)\n",
        "\n",
        "    X_valid, y_valid = df_dept[(df_dept['d'] >= 1914) & (df_dept['d'] < 1942)].drop('sold', axis=1), \\\n",
        "                       df_dept[(df_dept['d'] >= 1914) & (df_dept['d'] < 1942)]['sold']\n",
        "    valid_sets = lgb.Dataset(X_valid, y_valid)\n",
        "\n",
        "    X_test = df_dept[df_dept[\"d\"] >= 1942].drop(\"sold\", axis=1)\n",
        "\n",
        "    model = lgb.train(params=params,\n",
        "                      train_set=train_sets,\n",
        "                      valid_sets=valid_sets,\n",
        "                      verbose_eval=50,\n",
        "                      early_stopping_rounds=50)\n",
        "\n",
        "    pred_val = model.predict(X_valid)\n",
        "    valid.loc[X_valid.index, \"sold\"] = pred_val\n",
        "\n",
        "    pred_eva = model.predict(X_test)\n",
        "    test.loc[X_test.index, \"sold\"] = pred_eva\n",
        "\n",
        "    joblib.dump(model, f'model_dept_{store_id}_{dept_id}.pkl')\n",
        "    del model, X_train, y_train, X_valid, y_valid, X_test, train_sets, valid_sets\n",
        "    gc.collect()"
      ],
      "metadata": {
        "trusted": true,
        "id": "C2WGPGpdlv28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-68m0JOCn0to"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "store_dict ={}\n",
        "dept_dict = {}"
      ],
      "metadata": {
        "id": "RjZmH3Gglv29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(d_store)):\n",
        "    for j in range(len(d_dept)):\n",
        "      if store_dict.get(d_store[i],0) != 1:\n",
        "        store_preds_map = create_store_model(train_data, d_store[j])\n",
        "        store_dict[d_store[i]] = 1\n",
        "      if dept_dict.get(d_dept[j],0) != 1:\n",
        "        dept_preds_map = create_dept_model(train_data, d_dept[j])\n",
        "        dept_dict[d_dept[j],0] = 1\n",
        "      train_and_predict(data, i, j, store_preds_map, dept_preds_map)"
      ],
      "metadata": {
        "trusted": true,
        "id": "wNdFE17hlv29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "lmsaZN90lv29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = joblib.load(\"/kaggle/working/model_dept_0.pkl\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "3HO9q1Fxlv29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "# Get feature importances\n",
        "feature_importance = pd.DataFrame({'Feature': model.feature_name(), 'Importance': model.feature_importance(importance_type='split')})\n",
        "top_features = feature_importance.nlargest(15, 'Importance')\n",
        "\n",
        "# Plot feature importance with Seaborn styling\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.set(style=\"whitegrid\", font_scale=1.2)\n",
        "\n",
        "# Plot only the top 15 features with different colors\n",
        "sns.barplot(x='Importance', y='Feature', data=top_features, palette='viridis')\n",
        "\n",
        "# Customize plot further if needed\n",
        "plt.title('Top 15 Feature Importance (Split)')\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "GIRs1TALlv29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid = data[(data['d']>=1914) & (data['d']<1942)][['id','d','sold']]\n",
        "test = data[data['d']>=1942][['id','d','sold']]"
      ],
      "metadata": {
        "trusted": true,
        "id": "Agu4GORMlv2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "Oj1fXv3dlv2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid[\"id\"] = valid[\"id\"].map(d_id)\n",
        "valid = valid.pivot(index=\"id\", columns=\"d\", values=\"sold\").reset_index()\n",
        "valid[\"id\"] = valid[\"id\"].str.replace(\"evaluation\", \"validation\")\n",
        "\n",
        "submission = submission[[\"id\"]]\n",
        "\n",
        "f_col = [f\"F{i}\" for i in range(1,29)]\n",
        "f_col.insert(0, \"id\")\n",
        "\n",
        "out_val = pd.merge(left=submission[:30490], right=valid, on=\"id\")\n",
        "out_val.columns=f_col\n",
        "\n",
        "\n",
        "test[\"id\"] = test[\"id\"].map(d_id)\n",
        "test = test.pivot(index=\"id\", columns=\"d\", values=\"sold\").reset_index()\n",
        "\n",
        "out_eva = pd.merge(left=submission[30490:], right=test, on=\"id\")\n",
        "out_eva.columns=f_col\n",
        "\n",
        "\n",
        "submit = pd.concat([out_val,out_eva], ignore_index=True)"
      ],
      "metadata": {
        "trusted": true,
        "id": "7vmvIvMvlv2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit.to_csv('submission.csv',index=False)"
      ],
      "metadata": {
        "trusted": true,
        "id": "N9TDgap-lv2-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}