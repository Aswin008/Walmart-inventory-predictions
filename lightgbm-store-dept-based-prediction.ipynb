{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":18599,"databundleVersionId":1236839,"sourceType":"competition"},{"sourceId":7134990,"sourceType":"datasetVersion","datasetId":4116919}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nimport gc\nimport lightgbm as lgb\nimport joblib\nfrom lightgbm import LGBMRegressor","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"calendar = pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/calendar.csv\")\ntrain_data = pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/sales_train_evaluation.csv\")\nsell_prices = pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/sell_prices.csv\")\nsubmission = pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/sample_submission.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for d in range(1942,1970):\n    col = 'd_' + str(d)\n    train_data[col] = 0\n    train_data[col] = train_data[col].astype(np.int16)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def downcast(df):\n    cols = df.dtypes.index.tolist()\n    types = df.dtypes.values.tolist()\n    for i,t in enumerate(types):\n        if 'int' in str(t):\n            if df[cols[i]].min() > np.iinfo(np.int8).min and df[cols[i]].max() < np.iinfo(np.int8).max:\n                df[cols[i]] = df[cols[i]].astype(np.int8)\n            elif df[cols[i]].min() > np.iinfo(np.int16).min and df[cols[i]].max() < np.iinfo(np.int16).max:\n                df[cols[i]] = df[cols[i]].astype(np.int16)\n            elif df[cols[i]].min() > np.iinfo(np.int32).min and df[cols[i]].max() < np.iinfo(np.int32).max:\n                df[cols[i]] = df[cols[i]].astype(np.int32)\n            else:\n                df[cols[i]] = df[cols[i]].astype(np.int64)\n        elif 'float' in str(t):\n            if df[cols[i]].min() > np.finfo(np.float16).min and df[cols[i]].max() < np.finfo(np.float16).max:\n                df[cols[i]] = df[cols[i]].astype(np.float16)\n            elif df[cols[i]].min() > np.finfo(np.float32).min and df[cols[i]].max() < np.finfo(np.float32).max:\n                df[cols[i]] = df[cols[i]].astype(np.float32)\n            else:\n                df[cols[i]] = df[cols[i]].astype(np.float64)\n        elif t == object:\n            if cols[i] == 'date':\n                df[cols[i]] = pd.to_datetime(df[cols[i]], format='%Y-%m-%d')\n            else:\n                df[cols[i]] = df[cols[i]].astype('category')\n    return df  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Downcasting data\")\ntrain_eva = downcast(train_data)\nsell_prices = downcast(sell_prices)\ncalendar = downcast(calendar)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_pickle('data.pkl')\nvalid = data[(data['d']>=1914) & (data['d']<1942)][['id','d','sold']]\ntest = data[data['d']>=1942][['id','d','sold']]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Get Labels\")\nd_id = dict(zip(train_data[\"id\"].cat.codes, train_data[\"id\"]))\nd_store = dict(zip(train_data[\"store_id\"].cat.codes, train_data[\"store_id\"]))\nd_dept = dict(zip(train_data[\"dept_id\"].cat.codes, train_data[\"dept_id\"]))\nd_cat = dict(zip(train_data[\"cat_id\"].cat.codes, train_data[\"cat_id\"]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"data.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = data.drop(['sales_lag_5', 'sales_lag_8','sales_lag_10', 'sales_lag_25', 'sales_lag_26', 'sales_lag_27', 'rolling_mean_tmp_7_30',\n                  'rolling_mean_tmp_7_60', 'rolling_mean_tmp_14_7','rolling_mean_tmp_14_14', 'rolling_mean_tmp_14_30',\n                  'rolling_mean_tmp_14_60',  'rolling_mean_tmp_1_7','rolling_mean_tmp_1_14', 'rolling_mean_tmp_1_30',\n                  'rolling_mean_tmp_1_60', 'rolling_mean_tmp_7_7','rolling_mean_tmp_7_14', 'rolling_mean_tmp_7_30',\n                  'rolling_mean_tmp_7_60'], axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create store dataframe for model predict total sale for store\ndef create_dept_dataframe(train_eva, dept):\n    train = train_eva[train_eva[\"id\"].str.contains(dept)]\n  \n    \n    df = pd.melt(frame=train, \n                 id_vars=[\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"],\n                 var_name=\"d\", value_name=\"sold\")\n    sum_df = df.groupby([\"d\", \"dept_id\"])[\"sold\"].sum().reset_index()\n\n    df = pd.merge(left=df.drop(\"sold\", axis=1), right=sum_df, how=\"left\", on=[\"d\", \"dept_id\"])[[\"dept_id\", \"state_id\", \"d\", \"sold\"]]\n    \n    df.drop_duplicates(inplace=True)\n    \n    df = pd.merge(left=df, right=calendar, how=\"left\", on=\"d\")\n    \n    \n    df = df.drop([\"dept_id\", \"state_id\", \"date\", \"weekday\", \"event_name_2\", \"event_type_2\"], axis=1)\n    df[\"d\"] = df[\"d\"].str[2:].astype(np.int16)\n    \n    df[\"event_name_1\"] = df[\"event_name_1\"].cat.codes\n    df[\"event_type_1\"] = df[\"event_type_1\"].cat.codes\n    X_train, y_train = df[df[\"d\"] < 1914].drop(\"sold\", axis=1), df[df[\"d\"] < 1914][\"sold\"]\n    lgb_train_sets = lgb.Dataset(X_train, y_train)\n    X_valid, y_valid = df[(df[\"d\"] >= 1914) & (df[\"d\"] < 1942)].drop(\"sold\", axis=1), df[(df[\"d\"] >= 1914) & (df[\"d\"] < 1942)][\"sold\"]\n    lgb_valid_sets = lgb.Dataset(X_valid, y_valid)\n    X_test, y_test = df[df[\"d\"] >= 1942].drop(\"sold\", axis=1), df[df[\"d\"] >= 1942][\"sold\"] \n    \n    \n    return lgb_train_sets,lgb_valid_sets, X_train, y_train, X_valid, y_valid, X_test, y_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_store_dataframe(train, store):\n    train = train[train[\"id\"].str.contains(store)]\n    \n    final_df = pd.melt(frame=train, \n                 id_vars=[\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"],\n                 var_name=\"d\", value_name=\"sold\")\n    sum_df = final_df.groupby([\"d\", \"store_id\"])[\"sold\"].sum().reset_index()\n    final_df = pd.merge(left=final_df.drop(\"sold\", axis=1), right=sum_df, how=\"left\", on=[\"d\", \"store_id\"]) \\\n    [[\"store_id\", \"state_id\", \"d\", \"sold\"]]\n    final_df.drop_duplicates(inplace=True)\n    \n    final_df = pd.merge(left=final_df, right=calendar, how=\"left\", on=\"d\")\n    \n\n    final_df[\"d\"] = final_df[\"d\"].apply(lambda x: int(x[2:])).astype(np.int16)\n    \n    final_df[\"event_name_1\"] = final_df[\"event_name_1\"].cat.codes\n    final_df[\"event_type_1\"] = final_df[\"event_type_1\"].cat.codes\n    final_df = final_df.drop([\"store_id\", \"state_id\", \"date\", \"weekday\", \"event_name_2\", \"event_type_2\"], axis=1)\n    \n    X_train, y_train = final_df[final_df[\"d\"] < 1914].drop(\"sold\", axis=1), final_df[final_df[\"d\"] < 1914][\"sold\"]\n    lgb_train_sets = lgb.Dataset(X_train, y_train)\n    X_valid, y_valid = final_df[(final_df[\"d\"] >= 1914) & (final_df[\"d\"] < 1942)].drop(\"sold\", axis=1), final_df[(final_df[\"d\"] >= 1914) & \\\n                                                                                         (final_df[\"d\"] < 1942)][\"sold\"]\n    lgb_valid_sets = lgb.Dataset(X_valid, y_valid)\n    X_test, y_test = final_df[final_df[\"d\"] >= 1942].drop(\"sold\", axis=1), final_df[final_df[\"d\"] >= 1942][\"sold\"] \n    \n    \n    return lgb_train_sets,lgb_valid_sets, X_train, y_train, X_valid, y_valid, X_test, y_test","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {\"objective\" : \"tweedie\",\n        'boosting_type': 'gbdt',\n        'learning_rate': 0.034,\n        'max_depth' : 135,\n        'num_leaves': 88,\n        'n_estimators': 1000,\n        'force_row_wise':True,\n        'lambda_l2':1\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_dept_model(train_data, dept_id):\n    train_sets, valid_sets, X_train, y_train, X_valid, y_valid, X_test, y_test = create_dept_dataframe(train_data, dept_id)\n    new_index = y_test.index.map(lambda x: x + 1)\n    y_test.index = new_index\n\n    dept_model = lgb.train(params=params,\n                            train_set=train_sets,\n                            valid_sets=valid_sets,\n                            verbose_eval=50,\n                            early_stopping_rounds=50)\n\n    dept_preds = dept_model.predict(X_test)\n    dept_preds_map = dict(pd.Series(dept_preds, index=y_test.index))\n\n    return dept_preds_map","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_store_model(train_data, store_id):\n    train_sets, valid_sets, X_train, y_train, X_valid, y_valid, X_test, y_test = create_store_dataframe(train_data, store_id)\n    new_index = y_test.index.map(lambda x: x + 1)\n    y_test.index = new_index\n\n    store_model = lgb.train(params=params,\n                            train_set=train_sets,\n                            valid_sets=valid_sets,\n                            verbose_eval=50,\n                            early_stopping_rounds=50)\n\n    store_preds = store_model.predict(X_test)\n    store_preds_map = dict(pd.Series(store_preds, index=y_test.index))\n\n    return store_preds_map","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_and_predict(df, store_id, dept_id, store_preds_map, dept_preds_map):\n    df = data[(data[\"store_id\"] == store_id) & (data[\"dept_id\"] == dept_id)]\n    df.loc[df[\"d\"] >= 1942, \"total_sale_store\"] = df.loc[df[\"d\"] >= 1942, :][\"d\"].map(store_preds_map)\n    df.loc[df[\"d\"] >= 1942, \"total_sale_dept\"] = df.loc[df[\"d\"] >= 1942, :][\"d\"].map(dept_preds_map)\n\n    # Create train set\n    X_train, y_train = df_dept[df_dept['d'] < 1914].drop('sold', axis=1), df_dept[df_dept['d'] < 1914]['sold']\n    train_sets = lgb.Dataset(X_train, y_train)\n\n    X_valid, y_valid = df_dept[(df_dept['d'] >= 1914) & (df_dept['d'] < 1942)].drop('sold', axis=1), \\\n                       df_dept[(df_dept['d'] >= 1914) & (df_dept['d'] < 1942)]['sold']\n    valid_sets = lgb.Dataset(X_valid, y_valid)\n\n    X_test = df_dept[df_dept[\"d\"] >= 1942].drop(\"sold\", axis=1)\n\n    model = lgb.train(params=params,\n                      train_set=train_sets,\n                      valid_sets=valid_sets,\n                      verbose_eval=50,\n                      early_stopping_rounds=50)\n\n    pred_val = model.predict(X_valid)\n    valid.loc[X_valid.index, \"sold\"] = pred_val\n\n    pred_eva = model.predict(X_test)\n    test.loc[X_test.index, \"sold\"] = pred_eva\n\n    joblib.dump(model, f'model_dept_{store_id}_{dept_id}.pkl')\n    del model, X_train, y_train, X_valid, y_valid, X_test, train_sets, valid_sets\n    gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"store_dict ={}\ndept_dict = {}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(len(d_store)):\n    for j in range(len(d_dept)):\n        dept_preds_map = create_dept_model(train_data, d_dept[j])\n        store_preds_map = create_store_model(train_data, d_store[j])\n        train_and_predict(data, i, j, store_preds_map, dept_preds_map)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = joblib.load(\"/kaggle/working/model_dept_0.pkl\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\n\n# Get feature importances\nfeature_importance = pd.DataFrame({'Feature': model.feature_name(), 'Importance': model.feature_importance(importance_type='split')})\ntop_features = feature_importance.nlargest(15, 'Importance')\n\n# Plot feature importance with Seaborn styling\nplt.figure(figsize=(12, 8))\nsns.set(style=\"whitegrid\", font_scale=1.2)\n\n# Plot only the top 15 features with different colors\nsns.barplot(x='Importance', y='Feature', data=top_features, palette='viridis')\n\n# Customize plot further if needed\nplt.title('Top 15 Feature Importance (Split)')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid = data[(data['d']>=1914) & (data['d']<1942)][['id','d','sold']]\ntest = data[data['d']>=1942][['id','d','sold']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid[\"id\"] = valid[\"id\"].map(d_id)\nvalid = valid.pivot(index=\"id\", columns=\"d\", values=\"sold\").reset_index()\nvalid[\"id\"] = valid[\"id\"].str.replace(\"evaluation\", \"validation\")\n\nsubmission = submission[[\"id\"]]\n\nf_col = [f\"F{i}\" for i in range(1,29)]\nf_col.insert(0, \"id\")\n\nout_val = pd.merge(left=submission[:30490], right=valid, on=\"id\")\nout_val.columns=f_col\n\n\ntest[\"id\"] = test[\"id\"].map(d_id)\ntest = test.pivot(index=\"id\", columns=\"d\", values=\"sold\").reset_index()\n\nout_eva = pd.merge(left=submission[30490:], right=test, on=\"id\")\nout_eva.columns=f_col\n\n\nsubmit = pd.concat([out_val,out_eva], ignore_index=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit.to_csv('submission.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}